{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spylon-kernel"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the 'E:\\..Projectssss\\Python\\pySpark\\pysparkk\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading spylon-kernel-0.4.1.tar.gz (33 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: ipykernel in e:\\..projectssss\\python\\pyspark\\pysparkk\\lib\\site-packages (from spylon-kernel) (6.13.0)\n",
      "Requirement already satisfied: jedi>=0.10 in e:\\..projectssss\\python\\pyspark\\pysparkk\\lib\\site-packages (from spylon-kernel) (0.18.1)\n",
      "Collecting metakernel\n",
      "  Downloading metakernel-0.29.0-py2.py3-none-any.whl (216 kB)\n",
      "     -------------------------------------- 217.0/217.0 KB 1.3 MB/s eta 0:00:00\n",
      "Collecting spylon[spark]\n",
      "  Downloading spylon-0.3.0.tar.gz (140 kB)\n",
      "     -------------------------------------- 140.4/140.4 KB 8.1 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: tornado in e:\\..projectssss\\python\\pyspark\\pysparkk\\lib\\site-packages (from spylon-kernel) (6.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in e:\\..projectssss\\python\\pyspark\\pysparkk\\lib\\site-packages (from jedi>=0.10->spylon-kernel) (0.8.3)\n",
      "Requirement already satisfied: debugpy>=1.0 in e:\\..projectssss\\python\\pyspark\\pysparkk\\lib\\site-packages (from ipykernel->spylon-kernel) (1.6.0)\n",
      "Requirement already satisfied: packaging in e:\\..projectssss\\python\\pyspark\\pysparkk\\lib\\site-packages (from ipykernel->spylon-kernel) (21.3)\n",
      "Requirement already satisfied: nest-asyncio in e:\\..projectssss\\python\\pyspark\\pysparkk\\lib\\site-packages (from ipykernel->spylon-kernel) (1.5.5)\n",
      "Requirement already satisfied: ipython>=7.23.1 in e:\\..projectssss\\python\\pyspark\\pysparkk\\lib\\site-packages (from ipykernel->spylon-kernel) (8.4.0)\n",
      "Requirement already satisfied: traitlets>=5.1.0 in e:\\..projectssss\\python\\pyspark\\pysparkk\\lib\\site-packages (from ipykernel->spylon-kernel) (5.2.2.post1)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in e:\\..projectssss\\python\\pyspark\\pysparkk\\lib\\site-packages (from ipykernel->spylon-kernel) (7.3.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in e:\\..projectssss\\python\\pyspark\\pysparkk\\lib\\site-packages (from ipykernel->spylon-kernel) (0.1.3)\n",
      "Requirement already satisfied: psutil in e:\\..projectssss\\python\\pyspark\\pysparkk\\lib\\site-packages (from ipykernel->spylon-kernel) (5.9.1)\n",
      "Requirement already satisfied: jupyter-core>=4.9.2 in e:\\..projectssss\\python\\pyspark\\pysparkk\\lib\\site-packages (from metakernel->spylon-kernel) (4.10.0)\n",
      "Collecting pexpect>=4.8\n",
      "  Downloading pexpect-4.8.0-py2.py3-none-any.whl (59 kB)\n",
      "     ---------------------------------------- 59.0/59.0 KB 3.0 MB/s eta 0:00:00\n",
      "Collecting findspark\n",
      "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
      "Collecting pyyaml\n",
      "  Downloading PyYAML-6.0-cp310-cp310-win_amd64.whl (151 kB)\n",
      "     -------------------------------------- 151.7/151.7 KB 9.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in e:\\..projectssss\\python\\pyspark\\pysparkk\\lib\\site-packages (from ipython>=7.23.1->ipykernel->spylon-kernel) (0.4.4)\n",
      "Requirement already satisfied: pygments>=2.4.0 in e:\\..projectssss\\python\\pyspark\\pysparkk\\lib\\site-packages (from ipython>=7.23.1->ipykernel->spylon-kernel) (2.12.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in e:\\..projectssss\\python\\pyspark\\pysparkk\\lib\\site-packages (from ipython>=7.23.1->ipykernel->spylon-kernel) (3.0.29)\n",
      "Requirement already satisfied: backcall in e:\\..projectssss\\python\\pyspark\\pysparkk\\lib\\site-packages (from ipython>=7.23.1->ipykernel->spylon-kernel) (0.2.0)\n",
      "Requirement already satisfied: decorator in e:\\..projectssss\\python\\pyspark\\pysparkk\\lib\\site-packages (from ipython>=7.23.1->ipykernel->spylon-kernel) (5.1.1)\n",
      "Requirement already satisfied: stack-data in e:\\..projectssss\\python\\pyspark\\pysparkk\\lib\\site-packages (from ipython>=7.23.1->ipykernel->spylon-kernel) (0.2.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in e:\\..projectssss\\python\\pyspark\\pysparkk\\lib\\site-packages (from ipython>=7.23.1->ipykernel->spylon-kernel) (62.1.0)\n",
      "Requirement already satisfied: pickleshare in e:\\..projectssss\\python\\pyspark\\pysparkk\\lib\\site-packages (from ipython>=7.23.1->ipykernel->spylon-kernel) (0.7.5)\n",
      "Requirement already satisfied: entrypoints in e:\\..projectssss\\python\\pyspark\\pysparkk\\lib\\site-packages (from jupyter-client>=6.1.12->ipykernel->spylon-kernel) (0.4)\n",
      "Requirement already satisfied: pyzmq>=23.0 in e:\\..projectssss\\python\\pyspark\\pysparkk\\lib\\site-packages (from jupyter-client>=6.1.12->ipykernel->spylon-kernel) (23.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in e:\\..projectssss\\python\\pyspark\\pysparkk\\lib\\site-packages (from jupyter-client>=6.1.12->ipykernel->spylon-kernel) (2.8.2)\n",
      "Requirement already satisfied: pywin32>=1.0 in e:\\..projectssss\\python\\pyspark\\pysparkk\\lib\\site-packages (from jupyter-core>=4.9.2->metakernel->spylon-kernel) (304)\n",
      "Collecting ptyprocess>=0.5\n",
      "  Downloading ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in e:\\..projectssss\\python\\pyspark\\pysparkk\\lib\\site-packages (from packaging->ipykernel->spylon-kernel) (3.0.9)\n",
      "Requirement already satisfied: wcwidth in e:\\..projectssss\\python\\pyspark\\pysparkk\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel->spylon-kernel) (0.2.5)\n",
      "Requirement already satisfied: six>=1.5 in e:\\..projectssss\\python\\pyspark\\pysparkk\\lib\\site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel->spylon-kernel) (1.16.0)\n",
      "Requirement already satisfied: asttokens in e:\\..projectssss\\python\\pyspark\\pysparkk\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel->spylon-kernel) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in e:\\..projectssss\\python\\pyspark\\pysparkk\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel->spylon-kernel) (0.2.2)\n",
      "Requirement already satisfied: executing in e:\\..projectssss\\python\\pyspark\\pysparkk\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel->spylon-kernel) (0.8.3)\n",
      "Building wheels for collected packages: spylon-kernel, spylon\n",
      "  Building wheel for spylon-kernel (setup.py): started\n",
      "  Building wheel for spylon-kernel (setup.py): finished with status 'done'\n",
      "  Created wheel for spylon-kernel: filename=spylon_kernel-0.4.1-py2.py3-none-any.whl size=18370 sha256=1f271b2f57f1ff90eb2cec79a7c4b13f5d8180ad636bb436193dff779412b344\n",
      "  Stored in directory: c:\\users\\welcome\\appdata\\local\\pip\\cache\\wheels\\f3\\76\\78\\23b9e69e930f65ffd57dc9276c7ce33f9d130557ea8410a0ff\n",
      "  Building wheel for spylon (setup.py): started\n",
      "  Building wheel for spylon (setup.py): finished with status 'done'\n",
      "  Created wheel for spylon: filename=spylon-0.3.0-py3-none-any.whl size=136189 sha256=616802731527d3baec381f126bd71c4dd74565b1e84af3676a2326c3fbd31d9b\n",
      "  Stored in directory: c:\\users\\welcome\\appdata\\local\\pip\\cache\\wheels\\fe\\63\\90\\775bb4496e281806c467ad6a52de4701cd042ddf1a249c2121\n",
      "Successfully built spylon-kernel spylon\n",
      "Installing collected packages: spylon, ptyprocess, findspark, pyyaml, pexpect, metakernel, spylon-kernel\n",
      "Successfully installed findspark-2.0.1 metakernel-0.29.0 pexpect-4.8.0 ptyprocess-0.7.0 pyyaml-6.0 spylon-0.3.0 spylon-kernel-0.4.1\n"
     ]
    }
   ],
   "source": [
    "# !pip install pandas\n",
    "# !pip install spylon-kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Duplicacy Removal\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://Asus-Vivobook:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Duplicacy Removal</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2d8b5ababc0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o54.load.\n: java.lang.ClassNotFoundException: \nFailed to find data source: com.crealytics.spark.excel. Please find packages at\nhttp://spark.apache.org/third-party-projects.html\n       \r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedToFindDataSourceError(QueryExecutionErrors.scala:443)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:670)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:720)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:188)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.lang.ClassNotFoundException: com.crealytics.spark.excel.DefaultSource\r\n\tat java.net.URLClassLoader.findClass(Unknown Source)\r\n\tat java.lang.ClassLoader.loadClass(Unknown Source)\r\n\tat java.lang.ClassLoader.loadClass(Unknown Source)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:656)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:656)\r\n\tat scala.util.Failure.orElse(Try.scala:224)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:656)\r\n\t... 15 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32me:\\..Projectssss\\Python\\pySpark\\Duplicacy removal script\\script.ipynb Cell 3'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/..Projectssss/Python/pySpark/Duplicacy%20removal%20script/script.ipynb#ch0000002?line=0'>1</a>\u001b[0m df \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39;49mread\u001b[39m.\u001b[39;49mformat(\u001b[39m\"\u001b[39;49m\u001b[39mcom.crealytics.spark.excel\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49moption(\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/..Projectssss/Python/pySpark/Duplicacy%20removal%20script/script.ipynb#ch0000002?line=1'>2</a>\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mheader\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mTrue\u001b[39;49;00m)\u001b[39m.\u001b[39;49moption(\u001b[39m\"\u001b[39;49m\u001b[39minferSchema\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mtrue\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39mpart-00000-7eff43f4-d281-4fc8-bfab-52075be4537e-c000.xlsx\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/..Projectssss/Python/pySpark/Duplicacy%20removal%20script/script.ipynb#ch0000002?line=3'>4</a>\u001b[0m df\u001b[39m.\u001b[39mshow()\n",
      "File \u001b[1;32me:\\..Projectssss\\Python\\pySpark\\pysparkk\\lib\\site-packages\\pyspark\\sql\\readwriter.py:158\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[1;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[0;32m    157\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(path, \u001b[39mstr\u001b[39m):\n\u001b[1;32m--> 158\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_df(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jreader\u001b[39m.\u001b[39;49mload(path))\n\u001b[0;32m    159\u001b[0m \u001b[39melif\u001b[39;00m path \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    160\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(path) \u001b[39m!=\u001b[39m \u001b[39mlist\u001b[39m:\n",
      "File \u001b[1;32me:\\..Projectssss\\Python\\pySpark\\pysparkk\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[1;32me:\\..Projectssss\\Python\\pySpark\\pysparkk\\lib\\site-packages\\pyspark\\sql\\utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw):\n\u001b[0;32m    110\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 111\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[0;32m    112\u001b[0m     \u001b[39mexcept\u001b[39;00m py4j\u001b[39m.\u001b[39mprotocol\u001b[39m.\u001b[39mPy4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    113\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32me:\\..Projectssss\\Python\\pySpark\\pysparkk\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o54.load.\n: java.lang.ClassNotFoundException: \nFailed to find data source: com.crealytics.spark.excel. Please find packages at\nhttp://spark.apache.org/third-party-projects.html\n       \r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedToFindDataSourceError(QueryExecutionErrors.scala:443)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:670)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:720)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:188)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.lang.ClassNotFoundException: com.crealytics.spark.excel.DefaultSource\r\n\tat java.net.URLClassLoader.findClass(Unknown Source)\r\n\tat java.lang.ClassLoader.loadClass(Unknown Source)\r\n\tat java.lang.ClassLoader.loadClass(Unknown Source)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:656)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:656)\r\n\tat scala.util.Failure.orElse(Try.scala:224)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:656)\r\n\t... 15 more\r\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"com.crealytics.spark.excel\").option(\n",
    "    \"header\", True).option(\"inferSchema\", \"true\").load(\"part-00000-7eff43f4-d281-4fc8-bfab-52075be4537e-c000.xlsx\")\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.3 ('pysparkk': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b479e50572fd4ef80cb647eca6664c2b4cc5bdfcd0e7bbdaa29199c139c6e9de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
